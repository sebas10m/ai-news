<!DOCTYPE html>
<html>
<head>
    <title>Enforcing System 2 Level Thinking in Large Language Models</title>
</head>
<body>
    <h1>Enforcing System 2 Level Thinking in Large Language Models</h1>
    <p>For most of us, thinking deeply is not that straightforward and simple. Human brains have two modes of thinking: system-1 thinking, which is fast and intuitive, and system-2 thinking, which is slower but more rational.</p>
    <p>Large language models, like GPT-4, currently only have system-1 thinking, which can lead to errors and inaccuracies. To address this, several approaches can be taken to enforce system-2 thinking.</p>
    <h2>Prompt Engineering Strategies</h2>
    <p>One approach is through prompt engineering, which involves designing prompts to guide the model to think more critically and explore multiple possibilities.</p>
    <p>Chain of Thought is a simple and effective way to enforce system-2 thinking, where the model is asked to break down a problem into small steps and think through each step.</p>
    <p>Another approach is to use self-consistency with chain of thoughts (COTSC), which involves running chain of thought multiple times and reviewing and voting on answers that are most reasonable.</p>
    <p>Tree of Thoughts is an advanced prompting tactic that gets the model to come up with a few different ways to solve a problem and explore all the different branches and options.</p>
    <h2>Communicative Agents</h2>
    <p>Another approach is through the use of communicative agents, which allow for a multi-agent setup where users can define two different agents and simulate a conversation between them.</p>
    <p>Communicative agents can help identify flaws in the thinking process and provide feedback to the problem solver, leading to more accurate and reliable results.</p>
    <p>In this talk, we set up a group chat in AutoGen Studio with two agents, a problem solver and a reviewer, to resolve a complex problem. The problem solver initially came up with an incorrect answer, but the reviewer pointed out the flaws in the deduction process and the final arrangement. The problem solver then tried again and came up with a correct answer.</p>
    <p>This example demonstrates the effectiveness of communicative agents in enforcing system-2 thinking in large language models.</p>
    <h3>Conclusion</h3>
    <p>In conclusion, enforcing system-2 level thinking in large language models is crucial for accurate and reliable results. Through prompt engineering strategies and communicative agents, we can design more effective systems that can solve complex problems and improve our decision-making processes.</p>
</body>
</html>
