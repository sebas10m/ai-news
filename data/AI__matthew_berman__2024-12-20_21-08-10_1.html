<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Jailbreaking Frontier AI Systems</title>
</head>
<body>
    <h1>Anthropic's Jailbreaking Technique</h1>
    <p>Anthropic has developed a new jailbreak technique that is extremely easy to implement and breaks every frontier model out there, including text, vision, and audio models.</p>
    <h2>The Best of End Jailbreaking Technique</h2>
    <p>This technique, also known as shotgunning, is a simple black box algorithm that jailbreaks frontier AI systems across modalities without requiring access to the inner workings of the model.</p>
    <h2>How it Works</h2>
    <p>The technique works by repeatedly trying different variations of a prompt until the model gives the desired response. This is achieved by augmenting and decreasing the prompt with a combination of variations, including changing letter cases, adding misspellings, and modifying the text's appearance.</p>
    <h2>Effectiveness</h2>
    <p>The technique has been shown to be highly effective, with an attack success rate of 89% on GPT-4 and 78% on cloud 3.5 sonnet when sampling 10,000 augmented prompts.</p>
    <h2>Extending to Other Modalities</h2>
    <p>The technique has been applied to audio and vision models as well, with an attack success rate of 56% for GPT-4-0 vision and 72% for GPT-4-0 real-time API with audio inputs.</p>
    <h2>Power Law Scaling</h2>
    <p>The technique exhibits power law scaling behavior, meaning that the attack success rate increases with the number of sampled augmentations, allowing for effective scaling of jailbreaking efforts.</p>
    <h2>Combining with Other Jailbreaking Techniques</h2>
    <p>Combining the shotgunning jailbreak with other jailbreaking techniques enhances its effectiveness, making it a powerful tool for testing the limits of frontier AI systems.</p>
    <p>This technique has been open-sourced by Anthropic, allowing anyone to try it out and test its effectiveness on their own models.</p>
    <p>Bon shotgunning has the potential to uncover power law like scaling behavior for many models that predicts ASR, which is the attack success rate as a function of the number of sampled augmentations.</p>
    <p>The attack success rate is 50% on all eight models using N equals 10,000. That is, 10% of the time, 10,000 different variations result in a jailbreak. This means that any model can be jailbroken with enough attempts.</p>
    <p>It's essential to understand that these jailbreaks are not a bug, but rather a characteristic of how these models are designed to behave.</p>
    <p>Depending on your location, you may need to use these techniques to access censored information from models. Therefore, it's crucial to have this information in the public domain.</p>
</body>
</html>
